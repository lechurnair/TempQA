{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_qfdiNwUMgn9-9IyxLBJ-OJ50KZSumPt",
      "authorship_tag": "ABX9TyMxJq0GEA167dYhtqnwtZHN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lechurnair/TempQA/blob/main/validity_checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14s9BX3PUiic",
        "outputId": "b4ccc180-b279-48b5-fd2a-1ab8fed3f16d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lexical Approach"
      ],
      "metadata": {
        "id": "YsYnph3XVzaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter=0\n",
        "c=0\n",
        "\n",
        "def check_temporal_connotation(predicted_question, context):\n",
        "    global counter\n",
        "    global c\n",
        "    c+=1\n",
        "    doc = nlp(predicted_question)\n",
        "\n",
        "    context_doc = nlp(context)\n",
        "    words = []\n",
        "    for token in context_doc:\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\", \"VERB\"] and token.dep_ != \"aux\":\n",
        "            words.append(token.lemma_.lower())\n",
        "\n",
        "    flag = False\n",
        "    for word in words:\n",
        "        if word and (word in [token.lemma_.lower() for token in doc]):\n",
        "            counter += 1\n",
        "            flag = True\n",
        "            break\n",
        "\n",
        "    if not flag:\n",
        "        return False\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"advmod\":\n",
        "            if token.text.lower() in [\"how\", \"when\", \"how often\", \"how long\", \"how frequent\"]:\n",
        "                return True\n",
        "        elif token.dep_ == \"aux\" and token.text.lower() == \"when\":\n",
        "            return True\n",
        "        elif token.dep_ == \"prep\" and token.text.lower() in [\"during\", \"at\"]:\n",
        "            return True\n",
        "        elif token.text.lower() in [\"after\", \"before\", \"tomorrow\", \"yesterday\"]:\n",
        "            return True\n",
        "        elif token.dep_ == \"aux\" and (token.text.lower() == \"will\" or token.text.lower() == \"is\" or token.text.lower() == \"are\"):\n",
        "            return True\n",
        "        elif token.dep_ == \"ROOT\" and (token.text.lower() == \"is\" or token.text.lower() == \"are\"):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "#Use the required file. Each of the following files have questions generated from some dataset\n",
        "\n",
        "# data = pd.read_excel(\"/content/drive/MyDrive/generated_questions_sampling.xlsx\")\n",
        "# data = pd.read_csv(\"/content/drive/MyDrive/Aru/question_cnnDailyMail.csv\")\n",
        "# data = pd.read_csv(\"/content/drive/MyDrive/Aru/question_Samsum_test.csv\")\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/question_RealNews.csv\")\n",
        "\n",
        "filtered_data = data[data.apply(lambda row: check_temporal_connotation(row[\"predicted_question\"], row[\"context\"]), axis=1)]"
      ],
      "metadata": {
        "id": "K0NVHDIdzV3S",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Similarity"
      ],
      "metadata": {
        "id": "JtLPIkohWXYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter=0\n",
        "c=0\n",
        "def check_temporal_connotation(predicted_question, context):\n",
        "    global counter\n",
        "    global c\n",
        "    c += 1\n",
        "    doc = nlp(predicted_question)\n",
        "    context_doc = nlp(context)\n",
        "    noun_phrases = [chunk.text.lower() for chunk in context_doc.noun_chunks]\n",
        "\n",
        "    words = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\", \"VERB\"]]\n",
        "    phrases = [' '.join(words[i:i+2]) for i in range(len(words) - 1)]\n",
        "\n",
        "    flag = False\n",
        "    for phrase in phrases:\n",
        "        phrase_doc = nlp(phrase)\n",
        "        similarity_scores = [phrase_doc.similarity(nlp(np)) for np in noun_phrases]\n",
        "        max_score = max(similarity_scores, default=0)\n",
        "        #Can be adjusted\n",
        "        if max_score > 0.5:\n",
        "            counter += 1\n",
        "            flag = True\n",
        "            break\n",
        "\n",
        "    if not flag:\n",
        "        return False\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"advmod\":\n",
        "            if token.text.lower() in [\"how\", \"when\", \"how often\", \"how long\", \"how frequent\"]:\n",
        "                return True\n",
        "        elif token.dep_ == \"aux\" and token.text.lower() == \"when\":\n",
        "            return True\n",
        "        elif token.dep_ == \"prep\" and token.text.lower() in [\"during\", \"at\"]:\n",
        "            return True\n",
        "        elif token.text.lower() in [\"after\", \"before\", \"tomorrow\", \"yesterday\"]:\n",
        "            return True\n",
        "        elif token.dep_ == \"aux\" and (token.text.lower() == \"will\" or token.text.lower() == \"is\" or token.text.lower() == \"are\"):\n",
        "            return True\n",
        "        elif token.dep_ == \"ROOT\" and (token.text.lower() == \"is\" or token.text.lower() == \"are\"):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Aru/question_Samsum_test.csv\",nrows=200)\n",
        "filtered_data = data[data.apply(lambda row: check_temporal_connotation(row[\"predicted_question\"], row[\"context\"]), axis=1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPtLXyZdsiJi",
        "outputId": "a391d79a-2d1a-4d74-fff0-89c3924e4522",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-670a040d0cc5>:17: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_scores = [phrase_doc.similarity(nlp(np)) for np in noun_phrases]\n"
          ]
        }
      ]
    }
  ]
}