{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1URVglqVTEYfJK1aOe_9qqT6a5lYb8llV",
      "authorship_tag": "ABX9TyNnm6SngqQHieF7Bw4UFOFZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lechurnair/TempQA/blob/main/Question%20Generation/eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the quality of the generated question using BLEU SCORE and Embedding Similarity"
      ],
      "metadata": {
        "id": "1Iid_fuTTGVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ghsq_8cdBzo",
        "outputId": "32eb2247-4645-4dff-d817-3270d5f41c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 0.10038282322799107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def calculate_bleu_score(reference, candidate):\n",
        "    reference_tokens = [reference.split()]\n",
        "    candidate_tokens = candidate.split()\n",
        "\n",
        "    bleu_score = sentence_bleu(reference_tokens, candidate_tokens)\n",
        "    return bleu_score\n",
        "\n",
        "def calculate_bleu_scores(df):\n",
        "    bleu_scores = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        real_question = str(row['real_question'])\n",
        "        predicted_question = str(row['predicted_question'])\n",
        "\n",
        "        bleu_score = calculate_bleu_score(real_question, predicted_question)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    df['bleu_score'] = bleu_scores\n",
        "\n",
        "    average_bleu_score = df['bleu_score'].mean()\n",
        "\n",
        "    return average_bleu_score, df\n",
        "\n",
        "#The questions generated using BERT\n",
        "file_path = '/content/drive/MyDrive/generated_questions_sampling.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "average_bleu_score, df_with_bleu_scores = calculate_bleu_scores(df)\n",
        "\n",
        "print(f'Average BLEU Score: {average_bleu_score}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "def get_embedding(sentence):\n",
        "    tokens = nlp(sentence)\n",
        "    return np.mean([token.vector for token in tokens], axis=0)\n",
        "\n",
        "def calculate_cosine_similarity(reference_sentence, predicted_sentence):\n",
        "    reference_embedding = get_embedding(reference_sentence)\n",
        "    predicted_embedding = get_embedding(predicted_sentence)\n",
        "\n",
        "    reference_embedding = reference_embedding.reshape(1, -1)\n",
        "    predicted_embedding = predicted_embedding.reshape(1, -1)\n",
        "\n",
        "    similarity = cosine_similarity(reference_embedding, predicted_embedding)[0, 0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "file_path = '/content/drive/MyDrive/generated_questions_sampling.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "cosine_similarity_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    reference_question = str(row['real_question'])\n",
        "    predicted_question = str(row['predicted_question'])\n",
        "\n",
        "    similarity_score = calculate_cosine_similarity(reference_question, predicted_question)\n",
        "    cosine_similarity_scores.append(similarity_score)\n",
        "\n",
        "df['cosine_similarity'] = cosine_similarity_scores\n",
        "print(sum(cosine_similarity_scores)/len(cosine_similarity_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba-Z3liEi0pH",
        "outputId": "37c633d9-db34-4352-9010-51ccddee7bd8",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6523105222927896\n"
          ]
        }
      ]
    }
  ]
}