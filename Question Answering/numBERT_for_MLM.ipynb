{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "2LZGNSDCPUpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert word numbers to symbol numbers"
      ],
      "metadata": {
        "id": "GdvmMnyLPblv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CW1S9cTQnzs3"
      },
      "outputs": [],
      "source": [
        "# The majority of this code is to set up the numwords dict, which is only done on the first call.\n",
        "import re\n",
        "def text2int(textnum, numwords={}):\n",
        "    if not numwords:\n",
        "      units = [\n",
        "        \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
        "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
        "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
        "      ]\n",
        "\n",
        "      tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
        "\n",
        "      scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
        "\n",
        "      numwords[\"and\"] = (1, 0)\n",
        "      for idx, word in enumerate(units):    numwords[word] = (1, idx)\n",
        "      for idx, word in enumerate(tens):     numwords[word] = (1, idx * 10)\n",
        "      for idx, word in enumerate(scales):   numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
        "\n",
        "    # to remove hyphens between word numbers\n",
        "    textnum = re.sub(r'(\\b\\w+-\\w+\\b)', lambda match: match.group(0).replace('-', ' '), textnum)\n",
        "    textnum = re.sub(r'(\\b\\w+-\\w+\\b)', lambda match: match.group(0).replace('-', ' '), textnum)\n",
        "    # Preprocess the text to remove commas only between digits\n",
        "    textnum = re.sub(r',', '', textnum)\n",
        "    textnum = re.sub(r',', '', textnum)\n",
        "    textnum = re.sub(r'\\b(once|twice|thrice)\\b', lambda match: '1 time' if match.group(1) == 'once' else ('2 times' if match.group(1) == 'twice' else '3 times'), textnum)\n",
        "\n",
        "    resultStr = ''\n",
        "    current = result = 0\n",
        "    numFinish = True\n",
        "    for word in textnum.split():\n",
        "      # print(current)\n",
        "      # Code for converting time\n",
        "      if ':' in word:\n",
        "        # print(word)\n",
        "        if resultStr!='':\n",
        "            resultStr+=' '\n",
        "        try:\n",
        "          hours, minutes = map(int, word.split(':'))\n",
        "          minutes_fraction = minutes / 60\n",
        "          # print(minutes_fraction)\n",
        "          resultStr += f\"{hours}.{int((minutes_fraction)*100)}\"\n",
        "          numFinish = True\n",
        "          continue\n",
        "        except: pass\n",
        "\n",
        "      if word not in numwords:\n",
        "        if resultStr!='':\n",
        "            resultStr+=' '\n",
        "        if numFinish == False:\n",
        "          resultStr+=str(current)\n",
        "          resultStr+=' '\n",
        "        current = result = 0\n",
        "        numFinish = True\n",
        "        resultStr+= word\n",
        "        continue\n",
        "\n",
        "      numFinish = False\n",
        "      scale, increment = numwords[word]\n",
        "      current = current * scale + increment\n",
        "      if scale > 100:\n",
        "          result += current\n",
        "          current = 0\n",
        "    # Preprocess the text to replace plural forms with singular forms\n",
        "    resultStr = re.sub(r'\\b(hundreds|decades)\\b', lambda match: match.group(0).rstrip('s'), resultStr)\n",
        "\n",
        "    # Preprocess the text to convert decades to years\n",
        "    resultStr = re.sub(r'(\\b\\d+\\b)?\\s+decade\\b', lambda match: f\" {str(int(match.group(1) or 1) * 10)}\" + ' years', resultStr)\n",
        "\n",
        "    # Preprocess the text to convert centuries to years\n",
        "    resultStr = re.sub(r'(\\b\\d+\\b)?\\s+centur(y|ies)\\b', lambda match: f\" {str(int(match.group(1) or 1) * 100)}\" + ' years', resultStr)\n",
        "    resultStr = re.sub(r'  ', ' ', resultStr)\n",
        "    if(numFinish):\n",
        "      return resultStr\n",
        "    else:\n",
        "      return resultStr+' '+str(result)+str(current)\n",
        "\n",
        "# print(text2int(\"There are five-hundred-twenty-three apples and thirty-four oranges 2:54 am.\"))\n",
        "# print(text2int(\"This has been going on for 10 centuries\"))\n",
        "# print(text2int(\"I went there once twice thrice\"))\n",
        "\n",
        "# remove commas in numbers, convert time to numbers, hundreds to hungred, century to 100 years, 1 century to 100 years, decades to 10 years, twice to 2 times, 4:54 is 4.(54/60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert numbers to form expected by NumBERT"
      ],
      "metadata": {
        "id": "EK6yFvZHPiYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_pattern = re.compile(r\"(\\d+)\\.?(\\d*)\")\n",
        "def apply_scientific_notation(line):\n",
        "  \"\"\"Convert all numbers in a line to scientific notation.\"\"\"\n",
        "  return re.sub(number_pattern, number_repl, line)\n",
        "\n",
        "def number_repl(matchobj):\n",
        "  \"\"\"Given a matchobj from number_pattern, it returns a string writing the corresponding number in scientific notation.\"\"\"\n",
        "  pre = matchobj.group(1).lstrip(\"0\")\n",
        "  post = matchobj.group(2)\n",
        "  if pre and int(pre):\n",
        "    # number is >= 1\n",
        "    exponent = len(pre) - 1\n",
        "  else:\n",
        "    # find number of leading zeros to offset.\n",
        "    exponent = -re.search(\"(?!0)\", post).start() - 1\n",
        "    post = post.lstrip(\"0\")\n",
        "  return (pre + post).rstrip(\"0\") + \" scinotexp \" + str(exponent)\n"
      ],
      "metadata": {
        "id": "d5Ps6qeOoqMw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert numBERT checkpoints into pytorch weights"
      ],
      "metadata": {
        "id": "6cwG9M2_PsR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  coding=utf-8\n",
        "# Copyright 2018 The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# \"\"\"Convert BERT checkpoint.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "\n",
        "from transformers import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n",
        "    # Initialise PyTorch model\n",
        "    config = BertConfig.from_json_file(bert_config_file)\n",
        "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
        "    model = BertForPreTraining(config)\n",
        "\n",
        "    # Load weights from tf checkpoint\n",
        "    load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n",
        "\n",
        "    # Save pytorch-model\n",
        "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
        "    torch.save(model.state_dict(), pytorch_dump_path)\n",
        "\n",
        "convert_tf_checkpoint_to_pytorch(\"/content/drive/MyDrive/NumBERT/model.ckpt-1000000\",\n",
        "                                     \"/content/drive/MyDrive/NumBERT/bert_config.json\",\n",
        "                                     \"/content/drive/MyDrive/NumBERT/pytorch_model\")\n",
        "# convert_tf_checkpoint_to_pytorch(\"/content/drive/MyDrive/Colab Notebooks/NumBERT/model.ckpt-1000000\",\n",
        "#                                      \"/content/drive/MyDrive/Colab Notebooks/NumBERT/bert_config.json\",\n",
        "#                                      \"/content/drive/MyDrive/Colab Notebooks/NumBERT/pytorch_model\")"
      ],
      "metadata": {
        "id": "IkiL6NRqPxeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Code"
      ],
      "metadata": {
        "id": "dZpYSRvoP4NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForMaskedLM\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "__Z4l7QyoOS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data from Dataset"
      ],
      "metadata": {
        "id": "YK2a660gQFLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "# pip install pyarrow==11.0.0"
      ],
      "metadata": {
        "id": "i9eOEGh3Q7-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "5pdCrZsooJIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('mc_taco')"
      ],
      "metadata": {
        "id": "GJgQbWD5MKzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Event ordering and stationary cant be predicted in our model, so removing that data (category : 1,4 )\n",
        "\n",
        "updated_dataset = dict()\n",
        "\n",
        "updated_dataset['test'] = [x for x in dataset['test']  if (x['category'] not in [1,4]) and (x['label']!=0)]\n",
        "updated_dataset['validation'] = [x for x in dataset['validation'] if x['category'] not in [1,4] and (x['label']!=0)]\n",
        "\n",
        "# 2239 test and 826 validation\n",
        "# print(len(updated_dataset['test']))"
      ],
      "metadata": {
        "id": "wFQpcfeoO3WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Tokenizer using vocab file"
      ],
      "metadata": {
        "id": "mXxMpLDzQLOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bert-tensorflow"
      ],
      "metadata": {
        "id": "qAJgREgrQC8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XwRJpFWqQXjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert import tokenization\n",
        "# numbert vocab from their drive\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file='/content/vocab.txt', do_lower_case=True)"
      ],
      "metadata": {
        "id": "TaawU3PDQbNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Input"
      ],
      "metadata": {
        "id": "24QNJ6s7Qf1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LabelCategory(num):\n",
        "  if(num == 2):\n",
        "    return 'Typical Frequency'\n",
        "  elif(num == 3):\n",
        "    return 'Typical Time'\n",
        "  elif(num == 0):\n",
        "    return 'Event Duration'"
      ],
      "metadata": {
        "id": "qShxHYteO74b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from absl import flags\n",
        "sys.argv=['preserve_unused_tokens=False']\n",
        "flags.FLAGS(sys.argv)\n",
        "\n",
        "training_data = list()\n",
        "\n",
        "for x in updated_dataset['test']:\n",
        "  temp = dict()\n",
        "  for i in ['sentence','question','answer']:\n",
        "    temp[i] = tokenizer.tokenize(apply_scientific_notation(text2int(tokenization.convert_to_unicode(x[i]))))\n",
        "  temp['category'] = tokenizer.tokenize(LabelCategory(x['category']))\n",
        "  training_data.append(temp)\n"
      ],
      "metadata": {
        "id": "FVcLZN8CPCF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting into form that model expects"
      ],
      "metadata": {
        "id": "rwQiOuPMXBVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ConvertToModelInput(data):\n",
        "  max_sequence_length = 125\n",
        "  model_inputs = dict()\n",
        "  model_inputs['input_ids'] = []\n",
        "  model_inputs['labels'] = []\n",
        "  model_inputs['attention_mask'] = []\n",
        "  for temp in data:\n",
        "\n",
        "    # print(temp)\n",
        "    temp1 = dict()\n",
        "    temp2 = dict()\n",
        "    temp1['input_ids'] = temp['sentence']+temp['question']+['[MASK]']*len(temp['answer'])+['[']+temp['category']+[']']\n",
        "    temp2['input_ids'] = temp['sentence']+temp['question']+temp['answer']+['[']+['[MASK]']*len(temp['category'])+[']']\n",
        "    for i in [temp1,temp2]:\n",
        "      i_length = len(i['input_ids'])\n",
        "      i['labels'] = temp['sentence']+temp['question']+temp['answer']+['[']+temp['category']+[']']\n",
        "      if(i_length<max_sequence_length):\n",
        "        # print(len(i['input_ids']))\n",
        "        i['input_ids'] += (['[PAD]']*(max_sequence_length-i_length))\n",
        "        i['labels'] += (['[PAD]']*(max_sequence_length-i_length))\n",
        "      i['attention_mask'] = [1 if idx < i_length else 0 for idx in range(max_sequence_length)]\n",
        "      # print(len(i['input_ids']))\n",
        "\n",
        "      model_inputs['input_ids'].append(tf.convert_to_tensor(tokenizer.convert_tokens_to_ids(i['input_ids'])))\n",
        "      model_inputs['labels'].append(tf.convert_to_tensor(tokenizer.convert_tokens_to_ids(i['labels'])))\n",
        "      model_inputs['attention_mask'].append(tf.convert_to_tensor(i['attention_mask']))\n",
        "  model_inputs['input_ids'] = tf.convert_to_tensor(model_inputs['input_ids'])\n",
        "  model_inputs['labels'] = tf.convert_to_tensor(model_inputs['labels'])\n",
        "  model_inputs['attention_mask'] = tf.convert_to_tensor(model_inputs['attention_mask'])\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "a_ADio9jXE92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Finetuning"
      ],
      "metadata": {
        "id": "A50XjMcaQkma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import TFBertForMaskedLM\n",
        "\n",
        "# Define and compile your model\n",
        "class MyBertModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyBertModel, self).__init__()\n",
        "\n",
        "        # Load the pre-trained BERT model\n",
        "        self.bert = TFBertForMaskedLM.from_pretrained(\"/content/drive/MyDrive/NumBERT/pytorch_model\",from_pt=True, config = \"/content/drive/MyDrive/Colab Notebooks/NumBERT/bert_config.json\")\n",
        "\n",
        "        # Define additional layers or modifications to the BERT model if needed\n",
        "        # Example: self.dense_layer = tf.keras.layers.Dense(256, activation='relu')\n",
        "\n",
        "    def call(self, inputs, training=False, attention_mask = None):\n",
        "        # Forward pass through the BERT model\n",
        "        outputs = self.bert(inputs, training=training)\n",
        "        if(attention_mask):\n",
        "          outputs = self.bert(inputs, training=training, attention_mask = attention_mask)\n",
        "        return outputs\n",
        "\n",
        "inputs = ConvertToModelInput(training_data)\n",
        "model = MyBertModel()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "# Continue training with the custom checkpoint callback\n",
        "history = model.fit(\n",
        "    [inputs['input_ids'], inputs['attention_mask']],\n",
        "    inputs['labels'],\n",
        "    verbose=1,\n",
        "    batch_size=8,\n",
        "    epochs=10,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "metadata": {
        "id": "SiyMlEuoPGFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epochs vs loss"
      ],
      "metadata": {
        "id": "DxWoBf5wQoPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss vs. epoch curve as follows\n",
        "\n",
        "losses = history.history['loss']\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "ax1 = fig.add_subplot(111)\n",
        "ax1.plot(range(len(losses)),losses)\n",
        "ax1.set_xlabel(\"Epochs\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.set_title(\"Epoch vs Loss\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8238JYLHPLhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "B691lRP_TOtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ConvertToModelInputTest(data):\n",
        "  max_sequence_length = 125\n",
        "  model_inputs = dict()\n",
        "  model_inputs['input_ids'] = []\n",
        "  model_inputs['labels'] = []\n",
        "  model_inputs['attention_mask'] = []\n",
        "  for temp in data:\n",
        "\n",
        "    # print(temp)\n",
        "    temp1 = dict()\n",
        "    temp1['input_ids'] = temp['sentence']+temp['question']+['[MASK]']*len(temp['answer'])+['[']+temp['category']+[']']\n",
        "    for i in [temp1]:\n",
        "      i_length = len(i['input_ids'])\n",
        "      i['labels'] = temp['sentence']+temp['question']+temp['answer']+['[']+temp['category']+[']']\n",
        "      if(i_length<max_sequence_length):\n",
        "        # print(len(i['input_ids']))\n",
        "        i['input_ids'] += (['[PAD]']*(max_sequence_length-i_length))\n",
        "        i['labels'] += (['[PAD]']*(max_sequence_length-i_length))\n",
        "      i['attention_mask'] = [1 if idx < i_length else 0 for idx in range(max_sequence_length)]\n",
        "      # print(len(i['input_ids']))\n",
        "\n",
        "      model_inputs['input_ids'].append(tf.convert_to_tensor(tokenizer.convert_tokens_to_ids(i['input_ids'])))\n",
        "      model_inputs['labels'].append(tf.convert_to_tensor(tokenizer.convert_tokens_to_ids(i['labels'])))\n",
        "      model_inputs['attention_mask'].append(tf.convert_to_tensor(i['attention_mask']))\n",
        "  model_inputs['input_ids'] = tf.convert_to_tensor(model_inputs['input_ids'])\n",
        "  model_inputs['labels'] = tf.convert_to_tensor(model_inputs['labels'])\n",
        "  model_inputs['attention_mask'] = tf.convert_to_tensor(model_inputs['attention_mask'])\n",
        "  return model_inputs\n",
        "test_data = list()\n",
        "for x in updated_dataset['validation']:\n",
        "  temp = dict()\n",
        "  for i in ['sentence','question','answer']:\n",
        "    temp[i] = tokenizer.tokenize(apply_scientific_notation(text2int(tokenization.convert_to_unicode(x[i]))))\n",
        "  temp['category'] = tokenizer.tokenize(LabelCategory(x['category']))\n",
        "  # print(temp)\n",
        "  test_data.append(temp)\n",
        "input_test = ConvertToModelInputTest(test_data)"
      ],
      "metadata": {
        "id": "bAfqhzk2TSLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "outputs = dict()\n",
        "# outputs['predicted answer'] = []\n",
        "outputs['Experiment'] = []\n",
        "outputs['expected answer'] = []\n",
        "# print(len(inputs))\n",
        "prev_q = ''\n",
        "for i in range(len(input_test['input_ids'])):\n",
        "  if(updated_dataset['validation'][i]['question'] == prev_q ):\n",
        "    continue\n",
        "  prev_q = updated_dataset['validation'][i]['question']\n",
        "  # print(inputs['input_ids'][i].numpy())\n",
        "  mask_loc = np.where(inputs['input_ids'][i].numpy() == 103)[0].tolist()\n",
        "  # print(mask_loc)\n",
        "  out = model(tf.convert_to_tensor([input_test['input_ids'][i]])).logits[0].numpy()\n",
        "  predicted_token_ids = np.argmax(out, axis=1).tolist()\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
        "  non_pad_tokens = [token for token in predicted_tokens if token != '[PAD]']\n",
        "  outputs['Experiment'].append(' '.join(non_pad_tokens))\n",
        "  outputs['expected answer'].append(updated_dataset['validation'][i]['answer'])\n",
        "  # filtered_tokens = [predicted_tokens[token] for token in range(len(predicted_tokens)) if (predicted_tokens[token] != \"[PAD]\" and token in mask_loc)]\n",
        "  # outputs['predicted answer'].append(' '.join(filtered_tokens))\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(outputs)\n",
        "# Define the file name\n",
        "csv_file = \"outputs_NoRepeat.csv\"\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv(csv_file, index=False)\n",
        "print(f\"Dataset saved to {csv_file}\")"
      ],
      "metadata": {
        "id": "R89o0eUvnLE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}