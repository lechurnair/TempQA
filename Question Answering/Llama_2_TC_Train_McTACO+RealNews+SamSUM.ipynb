{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "tOcodNZD94S9",
        "LdHWH3m2_AS6",
        "Alv6w5Ha5gB5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GBLnf-gPeqd",
        "outputId": "afebb8bf-cfb8-4fc5-d06f-92bc8244c7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Te_wKsfP40k"
      },
      "outputs": [],
      "source": [
        "!pip install -q  torch peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 accelerate\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def LabelCategory(num):\n",
        "  if(num == 2):\n",
        "    return 'Frequency'\n",
        "  elif(num == 3):\n",
        "    return 'Typical Time'\n",
        "\n",
        "  elif(num == 0):\n",
        "    return 'Event Duration'\n",
        "  elif(num == 1):\n",
        "    return 'Event Ordering'\n",
        "  elif(num == 4):\n",
        "    return 'Stationarity'"
      ],
      "metadata": {
        "id": "XkUi2UiFe5wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "df1 = pd.read_csv('/content/outputs_Llama2.0_RealNews_2k.csv',nrows = 1500)\n",
        "df2 = pd.read_csv('/content/outputs_Llama2.0_final.csv',nrows = 1500)\n",
        "# Combine datasets by concatenating them vertically\n",
        "from datasets import Dataset\n",
        "data_name = 'mc_taco'\n",
        "training_data1 = load_dataset(data_name, split=\"test\")\n",
        "mctaco = [x for x in training_data1  if  (x['label']!=0)]\n",
        "\n",
        "# Event ordering and stationary cant be predicted in our model, so removing that data (category : 1,4 )\n",
        "# df3 = [x for x in training_data1  if  (x['label']!=0)]\n",
        "updated_dataset = pd.concat([df1, df2], ignore_index=True)"
      ],
      "metadata": {
        "id": "11h09GTVW5Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datas = list()\n",
        "for i in range(len(updated_dataset['context'])):\n",
        "    datas.append('<s>[INST] '+updated_dataset['context'][i]+' '+updated_dataset['question'][i]+' [ '+updated_dataset['category'][i]+' ]'+' [/INST] '+updated_dataset['answer'][i]+'</s>')\n",
        "for i in mctaco:\n",
        "  datas.append('<s>[INST] '+i['sentence']+' '+i['question']+' [ '+LabelCategory(i['category'])+' ]'+' [/INST] '+i['answer']+'</s>')\n"
      ],
      "metadata": {
        "id": "uNQNELbUNAy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "# Create the dataset directly from the list of strings\n",
        "training_data = Dataset.from_dict({\"text\": datas})\n",
        "\n",
        "# Model and tokenizer names\n",
        "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "refined_model = \"llama-2-TC\" #You can give it your own name\n",
        "\n",
        "# Tokenizer\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
        "\n",
        "# Quantization Config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "# Model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "icsUN6p6QQyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA Config\n",
        "peft_parameters = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Training Params\n",
        "train_params = TrainingArguments(\n",
        "    output_dir=\"./results_modified\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=training_data,\n",
        "    peft_config=peft_parameters,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=llama_tokenizer,\n",
        "    args=train_params\n",
        ")\n",
        "\n",
        "# Training\n",
        "fine_tuning.train()\n"
      ],
      "metadata": {
        "id": "-vb7MNOXeMSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test with McTACO"
      ],
      "metadata": {
        "id": "tOcodNZD94S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import I\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "data_name = 'mc_taco'\n",
        "# Generate Text\n",
        "# Event ordering and stationary cant be predicted in our model, so removing that data (category : 1,4 )\n",
        "training_data1 = load_dataset(data_name, split=\"validation\")\n",
        "updated_dataset_test = [x for x in training_data1  if (x['label']!=0)]\n",
        "datas = list()\n",
        "mc_taco_data = {'context':[],'question':[],'answer':[],'category':[]}\n",
        "prev = ''\n",
        "for temp in updated_dataset_test:\n",
        "\n",
        "    if(temp['question']==prev):\n",
        "      mc_taco_data['answer'][-1].append(temp['answer'])\n",
        "      continue\n",
        "    datas.append('<s>[INST] '+temp['sentence']+' '+temp['question']+' [ '+LabelCategory(temp['category'])+' ]'+' [/INST]')\n",
        "    mc_taco_data['context'].append(temp['sentence'])\n",
        "    mc_taco_data['question'].append(temp['question'])\n",
        "    mc_taco_data['answer'].append([temp['answer']])\n",
        "    mc_taco_data['category'].append(LabelCategory(temp['category']))\n",
        "    prev = temp['question']\n"
      ],
      "metadata": {
        "id": "IVApLpyiQ9V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_gen = pipeline(task=\"text-generation\", model=fine_tuning.model, tokenizer=llama_tokenizer, max_length=200)\n",
        "outputs = dict()\n",
        "outputs['ans'] = list()\n",
        "outputs['query'] = datas\n",
        "for i in datas:\n",
        "  output = text_gen(i)\n",
        "  outputs['ans'].append(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "qPBHM9JjtAGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tc = dict()\n",
        "tc['question'] = list()\n",
        "tc['answer'] = list()\n",
        "for line in outputs['ans']:\n",
        "    start_index = line.find(\"<s>\")\n",
        "    end_index = line.find(\"</s>\")\n",
        "\n",
        "    if start_index != -1 and end_index != -1:  # Both <s> and </s> tags are found in the line\n",
        "        parsed_text = line[start_index + 3:end_index]  # Extract the text between <s> and </s>\n",
        "        end_inst_index = parsed_text.find(\"[/INST]\")\n",
        "        tc['question'].append(parsed_text[7:end_inst_index])\n",
        "        tc['answer'].append(parsed_text[end_inst_index+7:])\n",
        "        print(tc['question'][-1],tc['answer'][-1])\n",
        "    elif start_index != -1:  # Only <s> tag is found\n",
        "        print(\"Incomplete tag sequence: <s> without </s>\")\n",
        "        continue\n",
        "    elif end_index != -1:  # Only </s> tag is found\n",
        "        print(\"Incomplete tag sequence: </s> without <s>\")\n",
        "        continue\n",
        "df = pd.DataFrame(tc)\n",
        "# Define the file name\n",
        "csv_file = \"outputs_Llama2.0_Trained on SamSUM test Mctaco.csv\"\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv(csv_file, index=False)\n",
        "print(f\"Dataset saved to {csv_file}\")"
      ],
      "metadata": {
        "id": "9KENQ5HSgZCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc['question'] = mc_taco_data['question']\n",
        "tc['expected answer'] = mc_taco_data['answer']\n",
        "tc['context'] = mc_taco_data['context']\n",
        "tc['category'] = mc_taco_data['category']"
      ],
      "metadata": {
        "id": "bN_brim5u7D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(tc)\n",
        "# Define the file name\n",
        "csv_file = \"Train_SamSUM_RealNews_McTACO_Test_Mctaco.csv\"\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv(csv_file, index=False)\n",
        "print(f\"Dataset saved to {csv_file}\")"
      ],
      "metadata": {
        "id": "bNi5sYAVvUhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}